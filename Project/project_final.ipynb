{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATS790 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import product\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit\n",
    "import itertools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART (Regression tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented the Classification and Regression Tree (CART) algorithm based on this blog  (ref: https://insidelearningmachines.com/build-a-decision-tree-in-python/). Implementing CART using object-oriented programming (OOP) was a logical choice, as it provides a clear and easily understandable structure. In my implementation, I created two classes: one for nodes and the other for the tree, which essentially comprises of nodes. The node class represents the decision points in the tree, and the tree class encapsulates the entire decision-making process. This OOP-based implementation of CART resulted in a highly organized and readable codebase, making it easy to modify and extend as needed. Additionally, I have added a leaf node ID system. This system assigns a unique ID to every newly created leaf node, making it easy to access individual nodes. This feature offers significant benefits to our gradient boosting algorithm as it allows us to access the leaf nodes quickly and efficiently. With this enhancement, our algorithm can operate at a higher level of performance, providing faster and more accurate results.\n",
    " \n",
    "The default CART implementation in scikit-learn cannot handle missing values, and traditional CART algorithms use surrogate splits to handle missing values. However, surrogate splits are computationally expensive and do not guarantee an improvement in effectiveness, as there may not be similar features to replace the missing values. To overcome this limitation, we adopted the naive approach used in xgboost and lightGBM, which directly assigns missing values to nodes with lower sum square error.\n",
    "\n",
    "Since CART is a binary tree, each split only considers two nodes. If there are n missing values, there are 2^n ways to assign them to these two nodes. For each missing value, there are two options: either it goes into the first node or the second node. Thus, the total number of ways to assign the n missing values is the product of the number of options for each value, which is 2 * 2 * ... * 2 (n times), or simply $2^n$. In our implementation, we simply iterate all possibile combinations and find the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    '''\n",
    "    Class of tree node\n",
    "    '''\n",
    "\n",
    "    def __init__(self, split_column = None, split_value = None, leaf_value = None, depth = None):\n",
    "        self.split_column = split_column  # split feature (None if the node is leaf node)\n",
    "        self.split_value = split_value    # split value in the current split feature (None if the node is leaf node)\n",
    "        self.leaf_value = leaf_value      # leaf value (None if the node is decision node)\n",
    "        self.depth = depth                # depth of the node\n",
    "        self.left = None                  # left child of the decision node (None if the node is leaf node)\n",
    "        self.right = None                 # right child of the decision node (None if the node is leaf node)\n",
    "        self.data = None                  # data points assigned to the leaf node (None if the node is decision node)\n",
    "        self.leaf_id = None               # leaf node ID (i.e 1,2,3...n) (None if the node is decision node)\n",
    "        \n",
    "    def update_parameters(self,split_column,split_value):\n",
    "        '''\n",
    "        Function to update the split feature and split value of the node \n",
    "\n",
    "        Inputs:\n",
    "            split_column - split feature\n",
    "            split_value  - split value in the current split feature\n",
    "        '''\n",
    "        self.split_column = split_column\n",
    "        self.split_value = split_value\n",
    "\n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Function to get parameters of the node \n",
    "\n",
    "        Outputs:\n",
    "            split_column - split feature\n",
    "            split_value  - split value in the current split feature\n",
    "        '''\n",
    "        return self.split_column, self.split_value\n",
    "    \n",
    "    def set_children(self, left_child, right_child):\n",
    "        '''\n",
    "        Function to set parameters of the node \n",
    "\n",
    "        Inputs:\n",
    "            left_child - left child of the decision node \n",
    "            split_value  - right child of the decision node \n",
    "        '''\n",
    "        self.left = left_child\n",
    "        self.right = right_child\n",
    "\n",
    "    def get_left_child(self):\n",
    "        '''\n",
    "        Function to get left child of the node \n",
    "\n",
    "        Outputs:\n",
    "            left_child - left child of the decision node \n",
    "        '''\n",
    "        return self.left\n",
    "    \n",
    "    def get_right_child(self):\n",
    "        '''\n",
    "        Function to get right child of the node \n",
    "\n",
    "        Outputs:\n",
    "            right_child - right child of the decision node \n",
    "        '''\n",
    "        return self.right\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    '''\n",
    "    Class of decision tree\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_depth, min_sample):\n",
    "        self.max_depth = max_depth     # maximum depth of the tree\n",
    "        self.min_sample = min_sample   # minimum number of samples of the tree\n",
    "        self.decisionTree = None       # tree structure (all nodes of the tree)\n",
    "        self.leaf_nodes = []           # a list records all leaf nodes\n",
    "        self.current_id = 0            # a vairable used for updating leaf ID (each leaf node have its unique ID start from 0)\n",
    "        self.ids = []                  # a list records all leaf nodes IDs\n",
    "        self.leaf_values = []          # a list records all leaf values (for fast access)\n",
    "    \n",
    "    \n",
    "    def sse(self, data):\n",
    "        '''\n",
    "        Sum of square error used for spliting node\n",
    "\n",
    "        Inputs:\n",
    "            data - current dataset\n",
    "        Ouputs:\n",
    "            sum of square error\n",
    "        '''\n",
    "        return np.sum((data[:,-1] - np.mean(data[:,-1]))**2)\n",
    "        \n",
    "    def leaf_value(self,data):\n",
    "        '''\n",
    "        Function to compute the leaf value of each leaf node (Mean of all response varaibles of the current dataset)\n",
    "\n",
    "        Inputs:\n",
    "            data - current dataset\n",
    "        Ouputs:\n",
    "            leaf value\n",
    "        '''\n",
    "        return np.mean(data[:,-1])\n",
    "    \n",
    "    def grow_tree(self, node, data, depth = 0):\n",
    "        '''\n",
    "        Recursive function to grow the tree                                 \n",
    "\n",
    "        Inputs:\n",
    "            node  - current node\n",
    "            data  - current dataset\n",
    "            depth - current depth of the tree (default start from 0)\n",
    "\n",
    "        '''\n",
    "\n",
    "        # If the not satisfy leaf node condition:\n",
    "        #  1. not reach max depth. \n",
    "        #  2. number of data points in current node lager than the threshold\n",
    "        # then keep spliting\n",
    "        if depth < self.max_depth - 1 and data.shape[0] > self.min_sample and len(np.unique(data[:,-1])) != 1:\n",
    "            num_features = data.shape[1] - 1  # Compute number of features\n",
    "            split_column = None                \n",
    "            split_value = None\n",
    "            left_data = None      # initialize left subset of data after split \n",
    "            right_data = None     # initialize right subset of data after split \n",
    "            split_Error = np.inf  # initialize total error after split\n",
    "            # looping all possible features/values\n",
    "            for feature in range(num_features):\n",
    "                for value in np.unique(data[:,feature]):\n",
    "                    # split the dataset into left and right based on current features/values\n",
    "                    left_indices = data[:, feature] <= value  \n",
    "                    right_indices = data[:, feature] > value\n",
    "                    left_data_temp = data[left_indices]\n",
    "                    right_data_temp = data[right_indices]\n",
    "                    # make sure the number of datapints in each node after split satisfy the threshold\n",
    "                    if left_data_temp.shape[0] >= self.min_sample and right_data_temp.shape[0] >= self.min_sample:\n",
    "                        # compute the error (impurity)\n",
    "                        err = self.sse(left_data_temp) + self.sse(right_data_temp)\n",
    "                        # update the error, split feature/value if it is better\n",
    "                        if err < split_Error:\n",
    "                            split_column = feature\n",
    "                            split_value = value\n",
    "                            split_Error = err\n",
    "                            left_data = left_data_temp\n",
    "                            right_data = right_data_temp\n",
    "            # If split successfully\n",
    "            if split_column is not None:\n",
    "                left_node = Node()   # Create new left child node object\n",
    "                right_node = Node()  # Create new right child node object\n",
    "                # Update parameters and connect left and right child to the current node\n",
    "                node.update_parameters(split_column,split_value)\n",
    "                node.set_children(left_node, right_node)\n",
    "                # Keep grow tree recursively for using left and right data after split \n",
    "                self.grow_tree(node.get_left_child(),left_data,depth+1)\n",
    "                self.grow_tree(node.get_right_child(),right_data,depth+1)\n",
    "            # If split not successfully, then set the node as leaf node\n",
    "            else: \n",
    "                # Make current node as leaf node\n",
    "                node.leaf_value = self.leaf_value(data)  # Compute the leaf value\n",
    "                node.data = data   # assign the data points belong to it                \n",
    "                node.leaf_id = self.current_id  # set the leaf ID\n",
    "                self.ids.append(self.current_id)  \n",
    "                self.leaf_values.append(node.leaf_value)\n",
    "                self.current_id += 1  # Update next ID\n",
    "                self.leaf_nodes.append(node)\n",
    "                return\n",
    "        # If reach the maximum depth, or number of data points in current node smaller or equal than the threshold\n",
    "        else:\n",
    "            # Make current node as leaf node\n",
    "            node.leaf_value = self.leaf_value(data)\n",
    "            node.data = data\n",
    "            node.leaf_id = self.current_id\n",
    "            self.ids.append(self.current_id)\n",
    "            self.leaf_values.append(node.leaf_value)\n",
    "            self.current_id += 1\n",
    "            self.leaf_nodes.append(node)\n",
    "            return\n",
    "\n",
    "    def grow_tree_missing(self, node, data, depth = 0):\n",
    "        '''\n",
    "        Recursive function to grow the tree specially used for handling dataset with missing value                                \n",
    "\n",
    "        Inputs:\n",
    "            node  - current node\n",
    "            data  - current dataset\n",
    "            depth - current depth of the tree (default start from 0)\n",
    "\n",
    "        '''\n",
    "\n",
    "        # If the not satisfy leaf node condition:\n",
    "        #  1. not reach max depth. \n",
    "        #  2. number of data points in current node lager than the threshold\n",
    "        # then keep spliting\n",
    "        if depth < self.max_depth - 1 and data.shape[0] > self.min_sample and len(np.unique(data[:,-1])) != 1:\n",
    "            num_features = data.shape[1] - 1  # Compute number of features\n",
    "            split_column = None                \n",
    "            split_value = None\n",
    "            left_data = None      # initialize left subset of data after split \n",
    "            right_data = None     # initialize right subset of data after split \n",
    "            split_Error = np.inf  # initialize total error after split\n",
    "            # looping all possible features/values\n",
    "            for feature in range(num_features):\n",
    "                #col_i = data[:,feature] # all feature values (column) include Nan\n",
    "                #col_i_no_na = col_i[~np.isnan(col_i)]  # filter Nan\n",
    "                has_nan_indices = np.where(np.isnan(data[:, feature]))[0]\n",
    "                no_nan_indices = np.where(~np.isnan(data[:, feature]))[0]\n",
    "                has_nan = data[has_nan_indices, :]\n",
    "                no_nan = data[no_nan_indices, :]    \n",
    "\n",
    "                for value in np.unique(no_nan[:,feature]):\n",
    "                    # split the dataset into left and right based on current features/values\n",
    "                    left_indices = data[:, feature] <= value  \n",
    "                    right_indices = data[:, feature] > value\n",
    "                    left_data_temp = data[left_indices]\n",
    "                    right_data_temp = data[right_indices]\n",
    "\n",
    "                    # Generate all possible combinations of assigning the new rows to the two dataframes\n",
    "                    combos = list(itertools.product([0, 1], repeat=has_nan.shape[0]))\n",
    "                    # Loop over each combination\n",
    "                    for combo in combos:\n",
    "                        # Create a copy of the original dataframes to modify\n",
    "                        df1_copy = left_data_temp.copy()\n",
    "                        df2_copy = right_data_temp.copy()\n",
    "\n",
    "                        # Assign the new rows to df1 or df2 based on the combo\n",
    "                        for i, row in enumerate(has_nan):\n",
    "                            if combo[i] == 0:\n",
    "                                df1_copy = np.vstack([df1_copy, row])\n",
    "                            else:\n",
    "                                df2_copy = np.vstack([df2_copy, row])\n",
    "\n",
    "                        # make sure the number of datapints in each node after split satisfy the threshold\n",
    "                        if df1_copy.shape[0] >= self.min_sample and df2_copy.shape[0] >= self.min_sample:\n",
    "                            # compute the error (impurity)\n",
    "                            err = self.sse(df1_copy) + self.sse(df2_copy)\n",
    "                            # update the error, split feature/value if it is better\n",
    "                            if err < split_Error:\n",
    "                                split_column = feature\n",
    "                                split_value = value\n",
    "                                split_Error = err\n",
    "                                left_data = df1_copy\n",
    "                                right_data = df2_copy\n",
    "            # If split successfully\n",
    "            if split_column is not None:\n",
    "                left_node = Node()   # Create new left child node object\n",
    "                right_node = Node()  # Create new right child node object\n",
    "                # Update parameters and connect left and right child to the current node\n",
    "                node.update_parameters(split_column,split_value)\n",
    "                node.set_children(left_node, right_node)\n",
    "                # Keep grow tree recursively for using left and right data after split \n",
    "                self.grow_tree(node.get_left_child(),left_data,depth+1)\n",
    "                self.grow_tree(node.get_right_child(),right_data,depth+1)\n",
    "            # If split not successfully, then set the node as leaf node\n",
    "            else: \n",
    "                # Make current node as leaf node\n",
    "                node.leaf_value = self.leaf_value(data)  # Compute the leaf value\n",
    "                node.data = data   # assign the data points belong to it                \n",
    "                node.leaf_id = self.current_id  # set the leaf ID\n",
    "                self.ids.append(self.current_id)  \n",
    "                self.leaf_values.append(node.leaf_value)\n",
    "                self.current_id += 1  # Update next ID\n",
    "                self.leaf_nodes.append(node)\n",
    "                return\n",
    "        # If reach the maximum depth, or number of data points in current node smaller or equal than the threshold\n",
    "        else:\n",
    "            # Make current node as leaf node\n",
    "            node.leaf_value = self.leaf_value(data)\n",
    "            node.data = data\n",
    "            node.leaf_id = self.current_id\n",
    "            self.ids.append(self.current_id)\n",
    "            self.leaf_values.append(node.leaf_value)\n",
    "            self.current_id += 1\n",
    "            self.leaf_nodes.append(node)\n",
    "            return\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        '''\n",
    "        Function to train the data to build the CART                               \n",
    "\n",
    "        Inputs:\n",
    "            X  - Predictor variables\n",
    "            y  - Response variables\n",
    "        '''\n",
    "\n",
    "        # Merge Predictor variables and Response variables into one dataset\n",
    "        data = np.concatenate((X,y.reshape(-1,1)),axis=1)  \n",
    "        # set the root node of the tree\n",
    "        self.decisionTree = Node()\n",
    "        # grow the tree\n",
    "        self.grow_tree(self.decisionTree,data,0)\n",
    "    \n",
    "    def fit_missing(self,X,y):\n",
    "        '''\n",
    "        Function to train the data to build the CART specially used for handling dataset with missing value                             \n",
    "\n",
    "        Inputs:\n",
    "            X  - Predictor variables\n",
    "            y  - Response variables\n",
    "        '''\n",
    "\n",
    "        # Merge Predictor variables and Response variables into one dataset\n",
    "        data = np.concatenate((X,y.reshape(-1,1)),axis=1)  \n",
    "        # set the root node of the tree\n",
    "        self.decisionTree = Node()\n",
    "        # grow the tree\n",
    "        self.grow_tree_missing(self.decisionTree,data,0)\n",
    "    \n",
    "    def find_leaf(self,decisionTree,x):\n",
    "        '''\n",
    "        Function to find the leaf node that the input data point belong to, and return the predict leaf value                      \n",
    "\n",
    "        Inputs:\n",
    "            decisionTree  - the trained CART\n",
    "            x             - one data point\n",
    "        Output:\n",
    "            leaf value\n",
    "        '''\n",
    "\n",
    "        # If it is not the leaf node, then traverse the tree recursively (go to child nodes)\n",
    "        if decisionTree.leaf_value == None:\n",
    "            split_column, split_value = decisionTree.get_parameters()\n",
    "            if x[split_column] <= split_value:\n",
    "                return self.find_leaf(decisionTree.get_left_child(),x)   # go left if smaller or equal than split value\n",
    "            else:\n",
    "                return self.find_leaf(decisionTree.get_right_child(),x)  # go right if greater than split value\n",
    "        # if it is leaf node, return leaf value\n",
    "        else:\n",
    "            return decisionTree.leaf_value\n",
    "    \n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Prediction function based on the trained CART                     \n",
    "\n",
    "        Inputs:\n",
    "            X  - predictor dataset\n",
    "        Output:\n",
    "            array of predicted values\n",
    "        '''\n",
    "\n",
    "        res = []\n",
    "        # go through each data points and make prediction\n",
    "        for i in range(X.shape[0]):\n",
    "            res.append(self.find_leaf(self.decisionTree,X[i]))\n",
    "        \n",
    "        return np.array(res).flatten()\n",
    "    \n",
    "\n",
    "    def find_leaf_id(self,decisionTree,x):\n",
    "        '''\n",
    "        Function to find leaf ID of a data point based on the trained CART                     \n",
    "\n",
    "        Inputs:\n",
    "            decisionTree  - the trained CART\n",
    "            x             - one data point\n",
    "        Output:\n",
    "            leaf ID\n",
    "        '''\n",
    "\n",
    "        # If it is not the leaf node, then traverse the tree recursively (go to child nodes)\n",
    "        if decisionTree.leaf_value == None:\n",
    "            split_column, split_value = decisionTree.get_parameters()\n",
    "            if x[split_column] <= split_value:\n",
    "                return self.find_leaf_id(decisionTree.get_left_child(),x)\n",
    "            else:\n",
    "                return self.find_leaf_id(decisionTree.get_right_child(),x)\n",
    "        # if it is leaf node, return leaf value\n",
    "        else:\n",
    "            return decisionTree.leaf_id\n",
    "\n",
    "    def apply(self,X):\n",
    "        '''\n",
    "        Function to find all leaf ID of a dataset based on the trained CART                     \n",
    "\n",
    "        Inputs:\n",
    "            X  - predictor dataset\n",
    "        Output:\n",
    "            array of leaf ID each data point belong to\n",
    "        '''\n",
    "        \n",
    "        res = []\n",
    "        # go through each data points and make prediction\n",
    "        for i in range(X.shape[0]):\n",
    "            res.append(self.find_leaf_id(self.decisionTree,X[i]))\n",
    "        \n",
    "        return np.array(res).flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial regression dataset\n",
    "In this section, we create an artificial regression dataset to evaluate and compare the performance of both our implemented CART model and the CART model from scikit-learn. I test them on 10 random regression dataset, we can see the performance are similar. The CART scikit-learn are slightly better thanour implemented CART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for our implemented CART in 10 random dataset [196.87157998975198, 206.79586271247052, 154.5985378655847, 120.73831934706669, 169.22009224003475, 198.79538375320283, 166.2302440144937, 125.31812261226318, 158.39924279843459, 181.20175540652903]\n",
      "RMSE for scikit-learn CART in 10 random dataset [208.85739802125946, 191.40829984128482, 140.46410584408977, 115.69282443677845, 177.5959540784621, 216.47153238147644, 177.39888598042444, 133.10915197915398, 146.43188358955544, 191.93575290183048]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_seeds = [random.randint(0, 10000) for i in range(10)]  # generate 10 random seeds\n",
    "\n",
    "# container to store the RMSE from our implemented CART\n",
    "rmse_custom = []\n",
    "# container to store the RMSE from scikit-learn CART\n",
    "rmse_scikit = []\n",
    "\n",
    "for i in random_seeds:\n",
    "    # generate regression dataset for random seed i, and do train/test split\n",
    "    X, y = make_regression(n_samples=100, n_features=10, random_state=i)\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    # train and predict our implemented CART\n",
    "    rgt = Tree(max_depth=5, min_sample = 3)\n",
    "    rgt.fit(X_train_reg,y_train_reg)\n",
    "    y_pred_reg = rgt.predict(X_test_reg)\n",
    "    rmse_custom.append(np.sqrt(mean_squared_error(y_test_reg,y_pred_reg)))  # append to list\n",
    "\n",
    "    # train and predict using CART from scikit learn\n",
    "    rgt_scikit = DecisionTreeRegressor(max_depth=5, min_samples_split = 3, min_samples_leaf = 3)\n",
    "    rgt_scikit.fit(X_train_reg,y_train_reg)\n",
    "    y_pred_reg_scikit = rgt_scikit.predict(X_test_reg)\n",
    "    rmse_scikit.append(np.sqrt(mean_squared_error(y_test_reg,y_pred_reg_scikit)))  # append to list\n",
    "\n",
    "print(\"RMSE for our implemented CART in 10 random dataset \" + str(rmse_custom))\n",
    "print(\"RMSE for scikit-learn CART in 10 random dataset \" + str(rmse_scikit))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Decision Tree (GBDT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented Gradient Boosting Decision Tree (GBDT) for binary classification based on this blog (reference:https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e). Additionally, I also implemented GBDT for multiclass classificaiton. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gradient_boosting(object):\n",
    "    '''\n",
    "    Class of gradient boosting decision tree (GBDT)\n",
    "    '''\n",
    "\n",
    "    def __init__(self,M,base_model=\"CART\",max_depth = 1,min_sample = 1,learning_rate = 0.1, method = \"binary classification\",loss = \"logistic\", missing_value = False,tol = None):\n",
    "        self.M = M                           # number of trees\n",
    "        self.base_model = base_model         # base classifier (default is CART)\n",
    "        self.max_depth = max_depth           # maximum depth of the base classifier (max depth for CART)\n",
    "        self.min_sample = min_sample         # minimum number of sample in a node\n",
    "        self.learning_rate = learning_rate   # learning rate \n",
    "        self.method = method                 # usage of gradient boosting (binary or multiclass classification)\n",
    "        self.loss = loss                     # loss function\n",
    "        self.missing_value = missing_value   # If there's missing value in the dataset\n",
    "        self.tol = tol                       # tolerance\n",
    "        self.trees = []                      # a list record M trees in generated in gradient boosting\n",
    "        self.num_class = None                # number of classes (this is only used in multiclass classification)\n",
    "        self.col_names = None                # column names\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        '''\n",
    "        Softmax function (used for multiclass classification)                    \n",
    "\n",
    "        Inputs:\n",
    "            x  - one dimensional array of data\n",
    "        Output:\n",
    "            probability of each element of the array\n",
    "        '''\n",
    "\n",
    "        e_x = math.e**(x - np.max(x, axis=1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "    def logit(self,x):\n",
    "        '''\n",
    "        Logit function (used for binary classification)                    \n",
    "\n",
    "        Inputs:\n",
    "            x  - one dimensional array of data\n",
    "        Output:\n",
    "            probability of each element of the array\n",
    "        '''\n",
    "        return expit(x)   #1/(1 + np.exp(-x))\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        '''\n",
    "        Function to train GBDT                              \n",
    "\n",
    "        Inputs:\n",
    "            x  - Predictor variables\n",
    "            y  - Response variables\n",
    "        '''\n",
    "\n",
    "        ##binary classification#############################################################################################\n",
    "\n",
    "        if self.method == \"binary classification\":\n",
    "\n",
    "            self.num_class = 2\n",
    "            # initial prediction\n",
    "            F0 = np.log(np.mean(y)/(1-np.mean(y)))\n",
    "            self.F0 = np.array([F0]*len(y))\n",
    "            # predict value after mth tree, we make it equal to the initial prediction, it will keep updating when grow a new tree\n",
    "            fm = self.F0.copy()\n",
    "    \n",
    "            # grow M trees\n",
    "            for m in range(self.M):\n",
    "                # compute residual\n",
    "                p = self.logit(fm)\n",
    "                r_im = y - p\n",
    "                if self.base_model == \"CART\":\n",
    "                    # fit a CART implemented in previous section using residual\n",
    "                    tree = Tree(max_depth = self.max_depth, min_sample = self.min_sample)  # set max depth and min number of sample for CART\n",
    "                    if self.missing_value == True:\n",
    "                        tree.fit_missing(x.values, r_im.values)  # train CART in missing value mode\n",
    "                    else:\n",
    "                        tree.fit(x.values, r_im.values)  # train CART in normal mode\n",
    "                    nodes = tree.apply(x.values)     # find corresponding leaf ID of each data point based on the trained CART\n",
    "                    \n",
    "                    # go through all leaf nodes\n",
    "                    for i in tree.ids:\n",
    "                        # find subset of data points belong to node i\n",
    "                        sub = np.equal(i, nodes)\n",
    "                        #compute the leaf value for node i\n",
    "                        denominator = (np.sum(p[sub]*(1-p[sub])))\n",
    "                        if np.abs(denominator) <= 1e-150:\n",
    "                            gamma = 0\n",
    "                        else:\n",
    "                            gamma = (np.sum(r_im[sub]))/denominator\n",
    "                        # update predict value for all data points belong to node i\n",
    "                        fm[sub] += self.learning_rate*gamma\n",
    "                        # update leaf value\n",
    "                        tree.leaf_values[i] = gamma\n",
    "                        tree.leaf_nodes[i].leaf_value = gamma\n",
    "                    # append current tree to the list of tree\n",
    "                    self.trees.append(tree)\n",
    "\n",
    "        ##multiclass classification#########################################################################################\n",
    "\n",
    "        one_hot = None # store one-hot encoding for response variable\n",
    "        if self.method == \"multiclass classification\":     \n",
    "            self.num_class = y.nunique() # count number of classes\n",
    "            one_hot = pd.get_dummies(y)  # one-hot encoding\n",
    "            self.col_names = one_hot.columns.tolist()  # get column names\n",
    "        \n",
    "            # initial prediction\n",
    "            F0 = []\n",
    "            for i in range(self.num_class):\n",
    "                coli = one_hot.iloc[:,i]   # get column for ith class in one-hot encoding\n",
    "                num_class_i = coli.value_counts()[1]  # count number of 1's \n",
    "                F0.append(np.array([num_class_i/len(y)]*len(y)))  # Calculate the proportion of ith class \n",
    "            \n",
    "            self.F0 = np.array(F0)\n",
    "            # predict value after mth tree, we make it equal to the initial prediction, it will keep updating when grow a new tree\n",
    "            fm = self.F0.copy()\n",
    "\n",
    "            # grow M trees\n",
    "            for m in range(self.M):\n",
    "                if self.base_model == \"CART\":\n",
    "                    # a list to store k trees for k classes \n",
    "                    ktrees = [] \n",
    "                    # compute softmax\n",
    "                    p = self.softmax(fm.T).T\n",
    "                    # go through all classes and fit CART for the residual\n",
    "                    for k in range(self.num_class):\n",
    "                        p_km = p[k] # predict value for kth tree\n",
    "                        r_km = np.array(one_hot.iloc[:,k]) - p_km # compute residual\n",
    "\n",
    "                        # fit kth CART tree\n",
    "                        tree = Tree(max_depth = self.max_depth, min_sample = self.min_sample)  # set max depth and min number of sample for CART\n",
    "                        if self.missing_value == True:\n",
    "                            tree.fit_missing(x.values, r_km)      # train CART\n",
    "                        else:\n",
    "                            tree.fit(x.values, r_km)      # train CART\n",
    "                        nodes = tree.apply(x.values)  # find corresponding leaf ID of each data point based on the trained CART\n",
    "\n",
    "                        # go through all leaf nodes\n",
    "                        for i in tree.ids:\n",
    "                            # find subset of data points belong to node i\n",
    "                            sub = np.equal(i, nodes)\n",
    "                            \n",
    "                            #compute the leaf value for node i\n",
    "                            denominator = (np.sum(np.abs(r_km[sub])*(1-np.abs(r_km[sub]))))\n",
    "                            if np.abs(denominator) <= 1e-150:   # if the denominator is too small, set gamma equal to 0. This is to avoid dividing zero error\n",
    "                                gamma = 0\n",
    "                            else:                            \n",
    "                                gamma = ((self.num_class-1)/self.num_class) * (np.sum(r_km[sub])/denominator)\n",
    "                            # update predict value for all data points belong to node i\n",
    "                            fm[k][sub] += self.learning_rate*gamma\n",
    "                            # update leaf value\n",
    "                            tree.leaf_values[i] = gamma\n",
    "                            tree.leaf_nodes[i].leaf_value = gamma\n",
    "                        ktrees.append(tree)\n",
    "                    self.trees.append(ktrees)\n",
    "\n",
    "    def predict_prob(self,x):\n",
    "        '''\n",
    "        Function to predict probabilities based on the GBDT                     \n",
    "\n",
    "        Inputs:\n",
    "            X  - predictor dataset\n",
    "        Output:\n",
    "            array of predicted probabilities\n",
    "        '''\n",
    "        \n",
    "        if self.method == \"binary classification\":\n",
    "\n",
    "            # important! resize the initial prediction as the input size\n",
    "            Fm = np.resize(self.F0, x.shape[0])\n",
    "            # go through each tree and adding up the predictions\n",
    "            for m in range(self.M):\n",
    "                Fm += self.learning_rate * self.trees[m].predict(x.values)\n",
    "            # Output the predict probabilities\n",
    "            prob = self.logit(Fm)\n",
    "        \n",
    "            return prob\n",
    "\n",
    "        if self.method == \"multiclass classification\":\n",
    "\n",
    "            # important! resize the initial prediction as the input size\n",
    "            F0_pred = []\n",
    "            for i in range(self.num_class):\n",
    "                F0_pred.append(np.resize(self.F0[i], x.shape[0]))\n",
    "            Fm = np.array(F0_pred)\n",
    "\n",
    "            # go through each tree and adding up the predictions\n",
    "            for m in range(self.M):\n",
    "                for k in range(self.num_class):\n",
    "                    Fm[k] += self.learning_rate * self.trees[m][k].predict(x.values)\n",
    "\n",
    "            # compute the probability\n",
    "            p = self.softmax(Fm.T).T\n",
    "            # store the probability in a dataframe\n",
    "            prob = pd.DataFrame(columns=self.col_names)\n",
    "            for k in range(self.num_class):\n",
    "                prob[prob.columns[k]] = p[k]\n",
    "\n",
    "            return prob\n",
    "\n",
    "\n",
    "    def predict(self,x):\n",
    "        '''\n",
    "        Function to predict labels based on the GBDT                     \n",
    "\n",
    "        Inputs:\n",
    "            X  - predictor dataset\n",
    "        Output:\n",
    "            array of prediction labels\n",
    "        '''\n",
    "        \n",
    "        if self.method == \"binary classification\":\n",
    "            # important! resize the initial prediction as the input size\n",
    "            Fm = np.resize(self.F0, x.shape[0])\n",
    "            # go through each tree and adding up the predictions\n",
    "            for m in range(self.M):\n",
    "                Fm += self.learning_rate * self.trees[m].predict(x.values)\n",
    "            # Output the predict probabilities\n",
    "            prob = self.logit(Fm)\n",
    "\n",
    "            # return the predict labels\n",
    "            return (prob > 0.5).astype(int)\n",
    "\n",
    "\n",
    "        if self.method == \"multiclass classification\":\n",
    "            # important! resize the initial prediction as the input size\n",
    "            F0_pred = []\n",
    "            for i in range(self.num_class):\n",
    "                F0_pred.append(np.resize(self.F0[i], x.shape[0]))\n",
    "            Fm = np.array(F0_pred)\n",
    "\n",
    "            # go through each tree and adding up the predictions\n",
    "            for m in range(self.M):\n",
    "                for k in range(self.num_class):\n",
    "                    Fm[k] += self.learning_rate * self.trees[m][k].predict(x.values)\n",
    "\n",
    "            # compute the probability\n",
    "            p = self.softmax(Fm.T).T\n",
    "            # store the probability in a dataframe\n",
    "            prob = pd.DataFrame(columns=self.col_names)\n",
    "            for k in range(self.num_class):\n",
    "                prob[prob.columns[k]] = p[k]\n",
    "\n",
    "            return self.max_col_name(prob)\n",
    "        \n",
    "\n",
    "    def max_col_name(self,df):\n",
    "        '''\n",
    "        Function to find the class with maximum probability as the prediction label                     \n",
    "\n",
    "        Inputs:\n",
    "            df  - probability dataframe generate in predict function\n",
    "        Output:\n",
    "            array of prediction labels\n",
    "        '''\n",
    "        \n",
    "        # Create a list to store the column names of the maximum values\n",
    "        col_names = []\n",
    "\n",
    "        # Loop through each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            # Find the name of the column with the maximum value in the row\n",
    "            max_col = row.idxmax()\n",
    "            # Append the column name to the list\n",
    "            col_names.append(max_col)\n",
    "\n",
    "        # Return the list of column names\n",
    "        return col_names\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the k-fold cross-validation function used for our implemented GBDT. Different metrics, such as accuracy, F1 score, and ROC, can be applied. Also, we used this in the later hyperparameter tuning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfoldcrossGBDT(trainx,trainy,M,max_depth,min_sample,learning_rate,method,kfold=10, missing_value = False, scoring = \"acc\"):\n",
    "    '''\n",
    "    Function to perform K-Fold Cross Validation for GBDT                      \n",
    "\n",
    "    Inputs:\n",
    "        trainx        -    predictor variables of training dataset\n",
    "        trainy        -    response variables of training dataset\n",
    "        M             -    Number of CART to grow\n",
    "        max_depth     -    maximum depth of the base classifier (max depth for CART)\n",
    "        min_sample    -    minimum number of sample in a node\n",
    "        learning_rate -    learning rate\n",
    "        method        -    binary or multiclass classification\n",
    "        kfold         -    Number of folds want to use\n",
    "        scoring       -    validity index (accuracy, adjusted rand index, f1 score, ROC)\n",
    "        \n",
    "    Output:\n",
    "        average score and its standard deviation\n",
    "    '''\n",
    "\n",
    "    kf = KFold(n_splits=kfold) # k splits\n",
    "    iter1 = kf.split(trainx)\n",
    "\n",
    "    # store the score for each split\n",
    "    acc = []\n",
    "    ari = []\n",
    "    f1 = []\n",
    "    roc_auc = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # next split:\n",
    "            i = next(iter1)\n",
    "            # split data\n",
    "            train_x = trainx.iloc[i[0]]\n",
    "            train_y = trainy.iloc[i[0]]\n",
    "            test_x =  trainx.iloc[i[1]]         \n",
    "            test_y =  trainy.iloc[i[1]] \n",
    "            #reset index\n",
    "            train_x.reset_index(drop=True, inplace=True)\n",
    "            test_x.reset_index(drop=True, inplace=True)\n",
    "            train_y.reset_index(drop=True, inplace=True)\n",
    "            test_y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            #train GBDT and predict\n",
    "            GBDT = gradient_boosting(M=M,max_depth = max_depth,min_sample = min_sample,learning_rate = learning_rate,method=method,missing_value = missing_value)\n",
    "            GBDT.fit(train_x,train_y)\n",
    "            y_pred = GBDT.predict(test_x)\n",
    "            \n",
    "            # record the score\n",
    "            acc.append(accuracy_score(y_pred, test_y))\n",
    "            ari.append(adjusted_rand_score(y_pred, test_y))\n",
    "            f1.append(f1_score(y_pred, test_y, average='weighted'))\n",
    "            if method == \"binary classification\":\n",
    "                fpr, tpr, thresholds = roc_curve(test_y, y_pred)\n",
    "                roc_auc.append(auc(fpr, tpr))\n",
    "            \n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    if scoring == \"acc\":\n",
    "        return [np.mean(acc),np.std(acc)]\n",
    "    if scoring == \"ari\":\n",
    "        return [np.mean(ari),np.std(ari)]\n",
    "    if scoring == \"f1\":\n",
    "        return [np.mean(f1),np.std(f1)]\n",
    "    if scoring == \"roc\":\n",
    "        return [np.mean(roc_auc),np.std(roc_auc)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv_GBDT(hyperparams, trainx,trainy, method, kfold=10, scoring = \"acc\"):\n",
    "    '''\n",
    "    Function to do hyperparameter tunning using grid search for GBDT                      \n",
    "\n",
    "    Inputs:\n",
    "        hyperparams:  -    dictionary of hyperparameters and their possible values\n",
    "        trainx        -    predictor variables of training dataset\n",
    "        trainy        -    response variables of training dataset\n",
    "        method        -    binary or multiclass classification\n",
    "        kfold         -    Number of folds want to use\n",
    "        scoring       -    validity index (accuracy, adjusted rand index, f1 score, ROC)\n",
    "        \n",
    "    Output:\n",
    "        Best parameter set and its score\n",
    "    '''\n",
    "\n",
    "    best_params = None  # Best parameter set\n",
    "    best_score = 0      # Best score\n",
    "\n",
    "    # Iterate over all combinations of hyperparameters\n",
    "    for params in product(*hyperparams.values()):\n",
    "\n",
    "        # Create a dictionary of hyperparameters\n",
    "        param_dict = dict(zip(hyperparams.keys(), params))\n",
    "        print(\"Current parameter set: \" + str(param_dict))\n",
    "\n",
    "        # We have 4 parameters need to tune in our implemented GBDT, get them from the current dictionary\n",
    "        M = param_dict.get('M')\n",
    "        max_depth = param_dict.get('max_depth')\n",
    "        min_sample = param_dict.get('min_sample')\n",
    "        learning_rate = param_dict.get('learning_rate')\n",
    "\n",
    "        # compute the cross validation score of current parameter set\n",
    "        score = kfoldcrossGBDT(trainx,trainy,M ,max_depth,min_sample,learning_rate,method = method,kfold=kfold, scoring = scoring)\n",
    "        print(str(scoring) + \":\" + str(score[0]))\n",
    "\n",
    "        # If it is better, then update it as the current best parameter set\n",
    "        if score[0] > best_score:\n",
    "            best_params = param_dict\n",
    "            best_score = score[0]\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Dataset (Binary classification)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Heat Disease Dataset was compiled in 1988 and comprises four separate databases: Cleveland, Hungary, Switzerland, and Long Beach V. It comprises 76 different attributes, including the predicted attribute, but in all published experiments, only a subset of 14 attributes were utilized. The dataset contains 1025 instances with 9 categorical attributes: Sex, Chest pain type, fasting blood sugar, thal, resting electrocardiographic result, maximum heart rate, exercise-induced angina, the slope, and the number of major vessels, and 4 numerical attributes: Age, Resting blood pressure, serum cholestoral, and ST depression. The \"target\" field specifically indicates whether a patient has heart disease, with a value of 0 representing the absence of the disease and 1 indicating its presence.\n",
    "\n",
    "dataset reference: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data preprocessing##\n",
    "\n",
    "df_heart = pd.read_csv('D:\\\\STATS4T06\\\\Datasets\\\\heartdf.csv', index_col=False)\n",
    "# drop first column which is the ID\n",
    "df_heart = df_heart.drop(df_heart.columns[0], axis=1)\n",
    "\n",
    "X = df_heart.drop('target', axis=1)\n",
    "y = df_heart['target']\n",
    "\n",
    "# normalize numerical attributes\n",
    "scaler = MinMaxScaler()\n",
    "num_cols = ['age','trestbps','chol','thalach','oldpeak']\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "# perform one-hot encoding for categorical attributes\n",
    "X = pd.get_dummies(X, columns=['slope','ca','thal'])\n",
    "\n",
    "\n",
    "# perform train-test split\n",
    "X_train_heart, X_test_heart, y_train_heart, y_test_heart = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "# reset index\n",
    "X_train_heart = X_train_heart.reset_index(drop=True)\n",
    "X_test_heart = X_test_heart.reset_index(drop=True)\n",
    "y_train_heart = y_train_heart.reset_index(drop=True)\n",
    "y_test_heart = y_test_heart.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current parameter set: {'M': 10, 'max_depth': 2, 'min_sample': 2, 'learning_rate': 0.1}\n",
      "acc:0.8228730822873082\n",
      "Current parameter set: {'M': 10, 'max_depth': 2, 'min_sample': 2, 'learning_rate': 0.5}\n",
      "acc:0.8688981868898186\n",
      "Current parameter set: {'M': 10, 'max_depth': 2, 'min_sample': 2, 'learning_rate': 1}\n",
      "acc:0.8423988842398885\n",
      "Current parameter set: {'M': 10, 'max_depth': 2, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.8228730822873082\n",
      "Current parameter set: {'M': 10, 'max_depth': 2, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.8688981868898186\n",
      "Current parameter set: {'M': 10, 'max_depth': 2, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.8423988842398885\n",
      "Current parameter set: {'M': 10, 'max_depth': 2, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.8228730822873082\n",
      "Current parameter set: {'M': 10, 'max_depth': 2, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.8688981868898186\n",
      "Current parameter set: {'M': 10, 'max_depth': 2, 'min_sample': 5, 'learning_rate': 1}\n",
      "acc:0.8423988842398885\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 2, 'learning_rate': 0.1}\n",
      "acc:0.8298465829846583\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 2, 'learning_rate': 0.5}\n",
      "acc:0.8786610878661087\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 2, 'learning_rate': 1}\n",
      "acc:0.8800557880055789\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.8298465829846583\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.8758716875871687\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.8953974895397491\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.8298465829846583\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.8870292887029289\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 1}\n",
      "acc:0.8814504881450489\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 2, 'learning_rate': 0.1}\n",
      "acc:0.9191073919107392\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 2, 'learning_rate': 0.5}\n",
      "acc:0.9470013947001394\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 2, 'learning_rate': 1}\n",
      "acc:0.9553695955369595\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.9177126917712691\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.9623430962343096\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.9595536959553695\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.8953974895397488\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.9665271966527196\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 1}\n",
      "acc:0.9637377963737795\n",
      "Current parameter set: {'M': 50, 'max_depth': 2, 'min_sample': 2, 'learning_rate': 0.1}\n",
      "acc:0.8591352859135286\n",
      "Current parameter set: {'M': 50, 'max_depth': 2, 'min_sample': 2, 'learning_rate': 0.5}\n",
      "acc:0.8744769874476988\n",
      "Current parameter set: {'M': 50, 'max_depth': 2, 'min_sample': 2, 'learning_rate': 1}\n",
      "acc:0.909344490934449\n",
      "Current parameter set: {'M': 50, 'max_depth': 2, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.8591352859135286\n",
      "Current parameter set: {'M': 50, 'max_depth': 2, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.8842398884239889\n",
      "Current parameter set: {'M': 50, 'max_depth': 2, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.902370990237099\n",
      "Current parameter set: {'M': 50, 'max_depth': 2, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.8591352859135286\n",
      "Current parameter set: {'M': 50, 'max_depth': 2, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.8800557880055787\n",
      "Current parameter set: {'M': 50, 'max_depth': 2, 'min_sample': 5, 'learning_rate': 1}\n",
      "acc:0.9093444909344491\n",
      "Current parameter set: {'M': 50, 'max_depth': 3, 'min_sample': 2, 'learning_rate': 0.1}\n",
      "acc:0.8786610878661089\n",
      "Current parameter set: {'M': 50, 'max_depth': 3, 'min_sample': 2, 'learning_rate': 0.5}\n",
      "acc:0.9483960948396094\n",
      "Current parameter set: {'M': 50, 'max_depth': 3, 'min_sample': 2, 'learning_rate': 1}\n",
      "acc:0.9553695955369595\n",
      "Current parameter set: {'M': 50, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.8814504881450489\n",
      "Current parameter set: {'M': 50, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.9483960948396094\n",
      "Current parameter set: {'M': 50, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.9609483960948396\n",
      "Current parameter set: {'M': 50, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.8786610878661089\n",
      "Current parameter set: {'M': 50, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.9539748953974896\n",
      "Current parameter set: {'M': 50, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 1}\n",
      "acc:0.9623430962343096\n",
      "Current parameter set: {'M': 50, 'max_depth': 6, 'min_sample': 2, 'learning_rate': 0.1}\n",
      "acc:0.9595536959553695\n",
      "Current parameter set: {'M': 50, 'max_depth': 6, 'min_sample': 2, 'learning_rate': 0.5}\n",
      "acc:0.9609483960948396\n",
      "Current parameter set: {'M': 50, 'max_depth': 6, 'min_sample': 2, 'learning_rate': 1}\n",
      "acc:0.9553695955369595\n",
      "Current parameter set: {'M': 50, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.9637377963737795\n",
      "Current parameter set: {'M': 50, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.9609483960948396\n",
      "Current parameter set: {'M': 50, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.9651324965132496\n",
      "Current parameter set: {'M': 50, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.9637377963737795\n",
      "Current parameter set: {'M': 50, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.9609483960948396\n",
      "Current parameter set: {'M': 50, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 1}\n",
      "acc:0.9483960948396094\n",
      "Current parameter set: {'M': 100, 'max_depth': 2, 'min_sample': 2, 'learning_rate': 0.1}\n",
      "acc:0.8730822873082288\n",
      "Current parameter set: {'M': 100, 'max_depth': 2, 'min_sample': 2, 'learning_rate': 0.5}\n",
      "acc:0.8981868898186889\n",
      "Current parameter set: {'M': 100, 'max_depth': 2, 'min_sample': 2, 'learning_rate': 1}\n",
      "acc:0.9177126917712691\n",
      "Current parameter set: {'M': 100, 'max_depth': 2, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.8716875871687587\n",
      "Current parameter set: {'M': 100, 'max_depth': 2, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.8981868898186889\n",
      "Current parameter set: {'M': 100, 'max_depth': 2, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.9191073919107392\n",
      "Current parameter set: {'M': 100, 'max_depth': 2, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.8702928870292888\n",
      "Current parameter set: {'M': 100, 'max_depth': 2, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.8981868898186889\n",
      "Current parameter set: {'M': 100, 'max_depth': 2, 'min_sample': 5, 'learning_rate': 1}\n",
      "acc:0.9246861924686192\n",
      "Current parameter set: {'M': 100, 'max_depth': 3, 'min_sample': 2, 'learning_rate': 0.1}\n",
      "acc:0.9079497907949791\n",
      "Current parameter set: {'M': 100, 'max_depth': 3, 'min_sample': 2, 'learning_rate': 0.5}\n",
      "acc:0.9609483960948396\n",
      "Current parameter set: {'M': 100, 'max_depth': 3, 'min_sample': 2, 'learning_rate': 1}\n",
      "acc:0.9567642956764296\n",
      "Current parameter set: {'M': 100, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.906555090655509\n",
      "Current parameter set: {'M': 100, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.9637377963737795\n",
      "Current parameter set: {'M': 100, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.9623430962343096\n",
      "Current parameter set: {'M': 100, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.9051603905160391\n",
      "Current parameter set: {'M': 100, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.9651324965132496\n",
      "Current parameter set: {'M': 100, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 1}\n",
      "acc:0.9679218967921895\n",
      "Current parameter set: {'M': 100, 'max_depth': 6, 'min_sample': 2, 'learning_rate': 0.1}\n",
      "acc:0.9581589958158996\n",
      "Current parameter set: {'M': 100, 'max_depth': 6, 'min_sample': 2, 'learning_rate': 0.5}\n",
      "acc:0.9595536959553695\n",
      "Current parameter set: {'M': 100, 'max_depth': 6, 'min_sample': 2, 'learning_rate': 1}\n",
      "acc:0.9651324965132496\n",
      "Current parameter set: {'M': 100, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.9651324965132496\n",
      "Current parameter set: {'M': 100, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.9609483960948396\n",
      "Current parameter set: {'M': 100, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.9595536959553695\n",
      "Current parameter set: {'M': 100, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.9623430962343096\n",
      "Current parameter set: {'M': 100, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.9595536959553695\n",
      "Current parameter set: {'M': 100, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 1}\n",
      "acc:0.9567642956764296\n"
     ]
    }
   ],
   "source": [
    "##Find best parameter set (Hyperparameter tuning)##\n",
    "parameters = {'M': [10,50,100], \n",
    "              'max_depth': [2,3,6], \n",
    "              'min_sample': [2,3,5],\n",
    "              'learning_rate':[0.1,0.5,1]}\n",
    "best_params_heart, best_score_heart = grid_search_cv_GBDT(parameters, X_train_heart,y_train_heart, method = \"binary classification\", kfold=3, scoring = \"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 100, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 1}\n",
      "0.9679218967921895\n"
     ]
    }
   ],
   "source": [
    "##Print the best parameter set##\n",
    "print(best_params_heart)\n",
    "print(best_score_heart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9902597402597403\n",
      "F1 score: 0.9902575829486129\n",
      "ROC_AUC: 0.9901315789473684\n",
      "============= Classification Report =================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       156\n",
      "           1       1.00      0.98      0.99       152\n",
      "\n",
      "    accuracy                           0.99       308\n",
      "   macro avg       0.99      0.99      0.99       308\n",
      "weighted avg       0.99      0.99      0.99       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Prediction on test set##\n",
    "gb_heart = gradient_boosting(M=100,max_depth = 3,min_sample = 5,learning_rate = 1,method=\"binary classification\")\n",
    "gb_heart.fit(X_train_heart,y_train_heart)\n",
    "\n",
    "# performance evaluation\n",
    "y_pred_heart_test = gb_heart.predict(X_test_heart)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test_heart, y_pred_heart_test)))\n",
    "print(\"F1 score: \" + str(f1_score(y_test_heart, y_pred_heart_test, average='weighted')))\n",
    "fpr, tpr, thresholds = roc_curve(y_test_heart, y_pred_heart_test)\n",
    "print(\"ROC_AUC: \" + str(auc(fpr, tpr)))\n",
    "print(\"============= Classification Report =================\")\n",
    "print(classification_report(y_test_heart, y_pred_heart_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credict Approval Dataset (Binary classification)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Credit Approval Dataset comprises financial and personal information of credit applicants, with 690 instances and 15 attributes. One noteworthy characteristic of this dataset is the diversity of its attributes, including continuous variables, nominal variables with small numbers of values, and nominal variables with larger numbers of values. After removing rows with missing values, 653 instances remain, with 296 instances having a positive credit score and 357 with a negative credit score. The number of numerical and categorical attributes is evenly distributed, with 9 categorical variables (A1, A4, A5, A6, A7, A9, A10, A12, A13) and 4 numerical attributes (A2, A3, A8, A11). All attribute names and values have been replaced with meaningless symbols to preserve the confidentiality of the data. We test the performance of our implemented GBDT for handling missing values using this dataset.\n",
    "\n",
    "dataset reference: https://archive.ics.uci.edu/ml/datasets/credit+approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset removing missing value: (690, 16)\n",
      "Size of dataset with missing value: (653, 16)\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "df_ca = pd.read_csv(\"D:\\\\STATS4T06\\\\Datasets\\\\crx.data\", delimiter=\",\",header=None)\n",
    "df_ca.columns = ['a0', 'a1', 'a2','a3','a4','a5','a6','a7','a8','a9','a10','a11','a12','a13','a14','a15'] # set column names\n",
    "df_ca = df_ca.replace(\"?\", np.nan)  # replace missing value as Nan\n",
    "\n",
    "##-------------------------Dataset with missing value---------------------------------#\n",
    "print(\"Size of dataset removing missing value: \" + str(df_ca.shape)) #print size of the dataframe\n",
    "X_n = df_ca.drop('a15', axis=1)\n",
    "y_n = df_ca['a15'].map({'+': 1, '-': 0})\n",
    "# normalize numerical attributes\n",
    "scaler = MinMaxScaler()\n",
    "num_cols = ['a1','a2','a7','a10','a13','a14']\n",
    "X_n[num_cols] = scaler.fit_transform(X_n[num_cols])\n",
    "# perform one-hot encoding for categorical attributes\n",
    "X_n = pd.get_dummies(X_n, columns=['a0','a3','a4','a5','a6','a8','a9','a11','a12'])\n",
    "# train/test split\n",
    "X_train_ca_n, X_test_ca_n, y_train_ca_n, y_test_ca_n = train_test_split(X_n, y_n, test_size=0.3, random_state=4)\n",
    "\n",
    "##-------------------------Dataset removing missing value------------------------------# \n",
    "df_ca = df_ca.dropna()\n",
    "print(\"Size of dataset with missing value: \" + str(df_ca.shape)) #print size of the dataframe\n",
    "X = df_ca.drop('a15', axis=1)\n",
    "y = df_ca['a15'].map({'+': 1, '-': 0})\n",
    "# normalize numerical attributes\n",
    "scaler = MinMaxScaler()\n",
    "num_cols = ['a1','a2','a7','a10','a13','a14']\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "# perform one-hot encoding for categorical attributes\n",
    "X = pd.get_dummies(X, columns=['a0','a3','a4','a5','a6','a8','a9','a11','a12'])\n",
    "# train/test split\n",
    "X_train_ca, X_test_ca, y_train_ca, y_test_ca = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first test the cross validation function that hanlding missing value, we can see the result is very good, but it is much slower than ignoring missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and standard deviation: [0.8654639175257731, 0.023461572512046523]\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation using dataset with missing value\n",
    "creditCv = kfoldcrossGBDT(X_train_ca_n,y_train_ca_n,M = 10,max_depth=6,min_sample=5,learning_rate=0.5,method=\"binary classification\",kfold=5, missing_value = True, scoring = \"acc\")\n",
    "print(\"Accuracy and standard deviation: \" + str(creditCv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and standard deviation: [0.8512183468705207, 0.03207403445393859]\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation using dataset removing missing value\n",
    "creditCv_no_na = kfoldcrossGBDT(X_train_ca,y_train_ca,M = 10,max_depth=6,min_sample=5,learning_rate=0.5,method=\"binary classification\",kfold=5, missing_value = False, scoring = \"acc\")\n",
    "print(\"Accuracy and standard deviation: \" + str(creditCv_no_na))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we aim to compare the performance of our approach to handling missing values against the approach of simply ignoring them. To achieve this, we will use the same test set for testing, and different training set. In one case, we will ignore the missing values, while in the other, we will use our approach to handle them. As result, we can see our approach to handling missing value has a much better performance than simply ignore it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT performance on same test set (trained using dataset with missing value): \n",
      "Accuracy: 0.8826530612244898\n",
      "============= Classification Report =================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       109\n",
      "           1       0.89      0.84      0.86        87\n",
      "\n",
      "    accuracy                           0.88       196\n",
      "   macro avg       0.88      0.88      0.88       196\n",
      "weighted avg       0.88      0.88      0.88       196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Train GBDT using dataset with missing value##\n",
    "gb_ca = gradient_boosting(M=10,max_depth = 3,min_sample = 5,learning_rate = 1,method=\"binary classification\",missing_value=True)\n",
    "gb_ca.fit(X_train_ca_n,y_train_ca_n)\n",
    "\n",
    "# performance evaluation\n",
    "y_pred_ca_n_test = gb_ca.predict(X_test_ca)\n",
    "print(\"GBDT performance on same test set (trained using dataset with missing value): \")\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test_ca, y_pred_ca_n_test)))\n",
    "print(\"============= Classification Report =================\")\n",
    "print(classification_report(y_test_ca, y_pred_ca_n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT performance on same test set (trained using dataset removing missing value) \n",
      "Accuracy: 0.8367346938775511\n",
      "============= Classification Report =================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       109\n",
      "           1       0.80      0.85      0.82        87\n",
      "\n",
      "    accuracy                           0.84       196\n",
      "   macro avg       0.83      0.84      0.84       196\n",
      "weighted avg       0.84      0.84      0.84       196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Train GBDT using dataset removing missing value##\n",
    "gb_ca = gradient_boosting(M=10,max_depth = 3,min_sample = 5,learning_rate = 1,method=\"binary classification\",missing_value=True)\n",
    "gb_ca.fit(X_train_ca,y_train_ca)\n",
    "\n",
    "# performance evaluation\n",
    "y_pred_ca_test = gb_ca.predict(X_test_ca)\n",
    "print(\"GBDT performance on same test set (trained using dataset removing missing value): \")\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test_ca, y_pred_ca_test)))\n",
    "print(\"============= Classification Report =================\")\n",
    "print(classification_report(y_test_ca, y_pred_ca_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine dataset (Multiclass classification)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used this dataset to test the multiclass classification performance of our implemented GBDT. Our implementation of GBDT achieved comparable results to the GBDT implementation in scikit-learn, but was slower in terms of computation speed.\n",
    "\n",
    "dateset reference: https://archive.ics.uci.edu/ml/datasets/wine+quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "df_wine = pd.read_csv('D:\\\\stats790\\\\winequality-red.csv')\n",
    "\n",
    "X = df_wine.drop('quality', axis=1)\n",
    "y = df_wine['quality']\n",
    "\n",
    "# normalize numerical predictors\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "# perform train-test split\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(X, y, test_size=0.3, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.5817547278303092\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.5924706107845643\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.5504871581906465\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.5790729619217991\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.5790458088423205\n",
      "Current parameter set: {'M': 10, 'max_depth': 3, 'min_sample': 5, 'learning_rate': 1}\n",
      "acc:0.557641196013289\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 0.1}\n",
      "acc:0.6022984283158701\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 0.5}\n",
      "acc:0.6041017122412471\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 3, 'learning_rate': 1}\n",
      "acc:0.5817738947099411\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 0.1}\n",
      "acc:0.5996230513672374\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 0.5}\n",
      "acc:0.6014087656529516\n",
      "Current parameter set: {'M': 10, 'max_depth': 6, 'min_sample': 5, 'learning_rate': 1}\n"
     ]
    }
   ],
   "source": [
    "##Find best parameter set (Hyperparameter tuning)##\n",
    "parameters = {'M': [10,50,100], \n",
    "              'max_depth': [3,6], \n",
    "              'min_sample': [3,5],\n",
    "              'learning_rate':[0.1,0.5,1]}\n",
    "best_params_wine, best_score_wine = grid_search_cv_GBDT(parameters, X_train_wine,y_train_wine, method = \"multiclass classification\", kfold=2, scoring = \"acc\")\n",
    "print(best_params_heart)\n",
    "print(best_score_heart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6333333333333333\n",
      "============= Classification Report =================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.14      0.07      0.09        15\n",
      "           5       0.70      0.72      0.71       211\n",
      "           6       0.59      0.64      0.62       190\n",
      "           7       0.66      0.49      0.56        59\n",
      "           8       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.63       480\n",
      "   macro avg       0.35      0.32      0.33       480\n",
      "weighted avg       0.63      0.63      0.63       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Prediction on test set##\n",
    "gb_wine = gradient_boosting(M=100,max_depth = 3,min_sample = 5,learning_rate = 0.1,method=\"multiclass classification\")\n",
    "gb_wine.fit(X_train_wine,y_train_wine)\n",
    "\n",
    "# performance evaluation\n",
    "y_pred_wine_test = gb_wine.predict(X_test_wine)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test_wine, y_pred_wine_test)))\n",
    "print(\"============= Classification Report =================\")\n",
    "print(classification_report(y_test_wine, y_pred_wine_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the performance of GBDT from scikit learn, we can see the result is similar, but it is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6375\n",
      "============= Classification Report =================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.12      0.07      0.09        15\n",
      "           5       0.71      0.70      0.70       211\n",
      "           6       0.61      0.68      0.64       190\n",
      "           7       0.63      0.44      0.52        59\n",
      "           8       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.64       480\n",
      "   macro avg       0.43      0.37      0.39       480\n",
      "weighted avg       0.64      0.64      0.63       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbdt = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=5)\n",
    "gbdt.fit(X_train_wine, y_train_wine)\n",
    "y_pred = gbdt.predict(X_test_wine)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_wine, y_pred))\n",
    "print(\"============= Classification Report =================\")\n",
    "print(classification_report(y_test_wine, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ab9e1c345c6ca72aee2a9dc5c191881083c1e0904cd418eb537d62ed6bc1fcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
